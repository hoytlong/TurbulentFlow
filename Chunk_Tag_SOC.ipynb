{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### Imports\n",
    "\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import neighbors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from __future__ import division\n",
    "import nltk, re, pprint\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import operator\n",
    "from pandas import Series, DataFrame\n",
    "from scipy import stats\n",
    "from numpy.random import permutation, shuffle\n",
    "import string\n",
    "\n",
    "import codecs\n",
    "import sys\n",
    "import os\n",
    "sys.stdout = codecs.getwriter('utf_8')(sys.stdout)\n",
    "sys.stdin = codecs.getreader('utf_8')(sys.stdin)\n",
    "sys.path.append('/Users/Hoyt/PythonScripts/Utils')\n",
    "from MLtools import *\n",
    "from RichertUtils import *\n",
    "\n",
    "#if not set, include path to where the postagger is, including name of actual .jar file\n",
    "os.environ['CLASSPATH'] = '.;C:\\Program Files (x86)\\Java\\jre7\\lib\\ext\\QTJava.zip;C:\\Users\\Public\\Documents\\MyData\\\n",
    "                                                PythonFiles\\stanford-postagger-full-2014-06-16\\stanford-postagger.jar'\n",
    "os.environ['JAVAHOME'] = 'c:\\Windows\\SysWOW64\\\\' #give path where your java.exe file is (e.g., C:\\Windows\\SysWOW64)\n",
    "\n",
    "from nltk.tag.stanford import POSTagger\n",
    "#be sure to use the appropriate model\n",
    "tagger = POSTagger('c:\\Users\\Public\\Documents\\MyData\\PythonFiles\\stanford-postagger-full-2014-06-16\\models\\english-left3words-distsim.tagger',\n",
    "                   'c:\\Users\\Public\\Documents\\MyData\\PythonFiles\\stanford-postagger-full-2014-06-16\\stanford-postagger.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import code for Rich's machine\n",
    "\n",
    "sys.path.append('/Users/richardjeanso/Dropbox/')\n",
    "from MLtools import *\n",
    "from Utils import *\n",
    "\n",
    "### Implement Stanford POS Tagger \n",
    "java_path = \"/Library/Java/JavaVirtualMachines/jdk1.7.0_60.jdk/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "path_to_model = \"/users/richardjeanso/stanford-tagger/stanford/models/english-bidirectional-distsim.tagger\"\n",
    "path_to_jar = \"/users/richardjeanso/stanford-tagger/stanford/stanford-postagger.jar\"\n",
    "tagger = nltk.tag.stanford.POSTagger(path_to_model, path_to_jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use this code to grab novels, chunk them, put into a data frame, and pickle for later use\n",
    "#note: if you've already done this, just unpickle the file in the next cell\n",
    "\n",
    "#cleaner functions\n",
    "import unicodedata\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    if input_str != '':\n",
    "        nkfd_form = unicodedata.normalize('NFKD', input_str)\n",
    "        return u\"\".join([c for c in nkfd_form if not unicodedata.combining(c)])\n",
    "\n",
    "#need to create seperate cleaner functions, cuz sometimes we want punct, and sometimes we don't\n",
    "#use this first function when chunking and storing chunks--gets rid or replaces things that cause unicode errors\n",
    "def clean_unicode_accents(raw):\n",
    "    import re\n",
    "    raw = re.sub(r'I\\.', '', raw)\n",
    "    raw = re.sub(r'\\]', '', raw)\n",
    "    raw = re.sub(r'II', '', raw)\n",
    "    raw = re.sub(r'\\[', '', raw)\n",
    "    raw = re.sub(r'\\*', '', raw)\n",
    "    raw = re.sub(r'[\\n\\r\\t]', ' ', raw)\n",
    "    raw = re.sub(u'\\u2013','-',raw)   #this is an em-dash\n",
    "    raw = re.sub(u'\\u2014','-',raw)   #also an em-dash\n",
    "    raw = re.sub(u'\\u2044','',raw)\n",
    "    raw = re.sub(u'\\u201c','\"',raw)   #this is left \" mark; just replacing them here\n",
    "    raw = re.sub(u'\\u201d','\"',raw)   #this is right \" mark\n",
    "    raw = re.sub(u'\\ufeff','',raw)\n",
    "    raw = re.sub(u'\\u2018','\\'',raw)  #this is left ' mark\n",
    "    raw = re.sub(u'\\u2019','\\'',raw)  #this is right ' mark\n",
    "    raw = re.sub(u'\\ufb02','',raw)\n",
    "    raw = re.sub(u'\\u2026', '', raw)\n",
    "    raw = re.sub(u'\\u2022', '', raw)\n",
    "    #these next items have to be marked as unicode since you're dealing with utf-8 strings\n",
    "    raw = re.sub(u'£', '', raw)\n",
    "    raw = re.sub(u'«', '', raw)\n",
    "    raw = re.sub(u'»', '', raw)\n",
    "    raw = re.sub(u'¢', '', raw)\n",
    "    raw = re.sub(u'æ', 'e', raw)\n",
    "    raw = re.sub(u'©', '', raw)\n",
    "    raw = remove_accents(raw)\n",
    "    return raw\n",
    "\n",
    "def cleaner(text):\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    text = re.sub(r'[,.;:\"?!*()\\']', '', text)\n",
    "    text = re.sub(r'-', ' ', text)\n",
    "    text = re.sub(r'[\\n\\t]', ' ', text)\n",
    "    #text = remove_accents(text)\n",
    "    return text\n",
    "\n",
    "#import the data frame for SOC corpus\n",
    "columns = ['book_id', 'libraries', 'novel_title', 'auth_last', 'auth_first', 'auth_dates', 'publ_city', 'publisher',\n",
    "           'publ_date', 'source', 'nationality', 'genre']\n",
    "\n",
    "#For Hoyt's Machine\n",
    "df = pd.read_csv(\"c:\\Users\\Hoyt\\Dropbox\\SOC_TEXTS\\SOC_TEXTS.csv\", names=columns)\n",
    "\n",
    "#For Rich's Machine\n",
    "#df = pd.read_csv(\"/Users/richardjeanso/Dropbox/SOC_TEXTS/SOC_TEXTS.csv\", names=columns)\n",
    "\n",
    "df = df.ix[1:]   #filter out first row (the column headers from original .csv)\n",
    "df['filepath'] = Series('',index=df.index)   #add column for filepaths\n",
    "df['chunk'] = Series('',index=df.index)    #add column for text chunks\n",
    "df['chunk_id'] = Series('1',index=df.index)    #not needed, but keeping for consistency with realism corpus\n",
    "\n",
    "\n",
    "#this is code from original pipeline (not necessary for the SOC texts)\n",
    "#set your chunk_length (in characters) -- this will impact following cell too, so treat as global variable\n",
    "#chunk_length = 1500\n",
    "\n",
    "#generate filepaths for each text we have and load in the texts, chunking as you go\n",
    "corpus_path = \"c:\\Users\\Hoyt\\Dropbox\\SOC_TEXTS\\\\\"\n",
    "#corpus_path = \"/Users/richardjeanso/Dropbox/SOC_TEXTS/\"\n",
    "\n",
    "for i in df.index:\n",
    "    df.filepath[i] = corpus_path + str(df.book_id[i]) + \".txt\"  #assign filepath\n",
    "    text = codecs.open(df.filepath[i], encoding=\"utf-8\", errors=\"ignore\")\n",
    "    raw = text.read()\n",
    "    raw = clean_unicode_accents(raw)\n",
    "    df.chunk[i] = raw\n",
    "\n",
    "#drop the columns we don't need\n",
    "df.drop(['libraries', 'auth_dates', 'publ_city', 'publisher', 'source', 'nationality', 'filepath', 'auth_first'], axis=1, inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now you're ready to send list of chunks for pos_tagging\n",
    "\n",
    "#function to POS tag a chunk of text\n",
    "def pos_tagger(chunk_list, tagset=\"stanford\"):\n",
    "    pos_chunks = []\n",
    "    for chunk in chunk_list:\n",
    "        word_list = []\n",
    "        sent_list = nltk.sent_tokenize(chunk)\n",
    "        for sent in sent_list:\n",
    "            words = nltk.word_tokenize(sent)\n",
    "            #this next little bit of code saves us from endless unicodeDecode errors!\n",
    "            word_list.append([el.encode('unicode_escape') for el in words])  \n",
    "        pos_chunks.append(word_list)\n",
    "    pos_chunks = map(tagger.tag_sents, pos_chunks)\n",
    "    if tagset == \"universal\":\n",
    "        pos_chunks = change_tagset(pos_chunks)\n",
    "    return pos_chunks\n",
    "\n",
    "def change_tagset(tagged_corpus):\n",
    "    new_tagged_corpus = []\n",
    "    for chunk in tagged_corpus:\n",
    "        new_chunk = []\n",
    "        for sent in chunk:\n",
    "            new_sent = []\n",
    "            for item in sent:\n",
    "                if item[1].startswith('N'):         #include an if statement for every tag you want to change\n",
    "                    new_sent.append((item[0], 'N'))\n",
    "                elif item[1].startswith('V'):\n",
    "                    new_sent.append((item[0], 'V'))\n",
    "                elif item[1].startswith('J'):\n",
    "                    new_sent.append((item[0], 'J'))\n",
    "                else:\n",
    "                    new_sent.append(item)\n",
    "            new_chunk.append(new_sent)\n",
    "        new_tagged_corpus.append(new_chunk)\n",
    "    return new_tagged_corpus\n",
    "\n",
    "#get the chunks\n",
    "chunk_list = soc_chunks_df.chunk.values\n",
    "#pos tag them\n",
    "pos_chunk_list = pos_tagger(chunk_list, \"universal\")       \n",
    "#incoporate into the data frame\n",
    "soc_chunks_df['pos_chunk'] = Series(pos_chunk_list, index=soc_chunks_df.index)\n",
    "#pickle it\n",
    "soc_chunks_df.to_pickle(r\"c:\\Users\\Hoyt\\Dropbox\\SOC_TEXTS\\soc_chunks_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 8)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now build the feature sets for the soc_chunks \n",
    "#note: because these are cleaner chunks, I've adapted the code so it doesn't cut off front and end fragments\n",
    "soc_chunks_df = pd.read_pickle(r\"c:\\Users\\Hoyt\\Dropbox\\SOC_PROJECT\\REALISM_TEXTS\\soc_chunks_df.pkl\")\n",
    "soc_chunks_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215480"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build the dictionary\n",
    "\n",
    "#code for neologism detector (expanded version -- all words)\n",
    "with codecs.open('c:\\Users\\Hoyt\\Dropbox\\SOC_PROJECT\\WEB_DICT_1913.txt','r', 'utf-8') as f:\n",
    "    raw = f.read()\n",
    "\n",
    "#with codecs.open('/Users/richardjeanso/Dropbox/SOC_PROJECT/WEB_DICT_1913.txt','r', 'utf-8') as f:\n",
    "#    raw = f.read()\n",
    "\n",
    "raw = raw[1:]   #get rid of initial unicode character\n",
    "Xdict_words = re.split(' ', raw)\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nkfd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return u\"\".join([c for c in nkfd_form if not unicodedata.combining(c)])\n",
    "\n",
    "Xdict_entries = []\n",
    "for word in Xdict_words:\n",
    "    word = ''.join(c for c in word if c not in string.punctuation)\n",
    "    word = re.sub(r'-', '', word)\n",
    "    if len(word) > 1 and re.search(r'[0-9]', word) != True and re.search(r'[AEIOUYaeiouy]', word):\n",
    "        word = word.lower()\n",
    "        word = remove_accents(word)\n",
    "        Xdict_entries.append(word)\n",
    "\n",
    "new_entries = [\"a\", \"o\", \"i\", \"couldn\", \"mustn\", \"wasn\", \"wouldn\", \"ain\", \"ll\", \"needn\", \"shouldn\", \"needn\", \n",
    "               \"oughtn\", \"hasn\", \"hadn\", \"didn\", \"doesn\", \"don't\", \"n't\", \"didn't\", \"'s\", \"'d\", \"'ll\", \"'m\", ]\n",
    "\n",
    "for el in new_entries:\n",
    "    Xdict_entries.append(el)\n",
    "\n",
    "# Add in British spellings of words, i.e. \"colour\" and \"aeroplane\"\n",
    "with codecs.open('c:\\Users\\Hoyt\\Dropbox\\SOC_PROJECT\\BRITISH_SPELLING.txt','r', 'utf-8') as f:\n",
    "    raw = f.read()\n",
    "    british = re.split('\\n|\\r', raw)\n",
    "\n",
    "for word in british:\n",
    "    Xdict_entries.append(word)\n",
    "\n",
    "Xdict_entries = list(set(Xdict_entries))\n",
    "Xdict_entries.sort()         #do this for more efficient lookup in later function\n",
    "len(Xdict_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#functions for adding additional features to the data frame\n",
    "def median_sent_length(chunk):\n",
    "    chunk_sentences = nltk.sent_tokenize(chunk)\n",
    "    sent_length = []\n",
    "    for sent in chunk_sentences:\n",
    "        sent_length.append(len(sent))\n",
    "    return np.median(sent_length)\n",
    "\n",
    "#returns the percentage of sentences in chunk that end with noun\n",
    "def noun_ending(pos_chunk):\n",
    "    #pos_chunk = pos_chunk[1:-1]  #eliminate first and last items, since these are likely fragments\n",
    "    noun_endings = 0\n",
    "    total_sents = 0\n",
    "    for sent in pos_chunk:\n",
    "        if len(sent) > 2:           #get rid of obvious non-sentences\n",
    "            tag_index = -1          #we want to start from the end of the sentence\n",
    "            final_tag = sent[tag_index][1]\n",
    "            while re.search('[^A-Z$]+', final_tag) and abs(tag_index) != len(sent):    #while tag not punctuation...\n",
    "                tag_index = tag_index - 1             #step back one element\n",
    "                final_tag = sent[tag_index][1]\n",
    "            if final_tag == 'N':                      #keep track of nouns\n",
    "                noun_endings += 1\n",
    "            total_sents += 1\n",
    "    if total_sents == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return noun_endings/total_sents               #normalize \n",
    "\n",
    "#returns the percentage of verbless sentences in each chunk\n",
    "def verbless_sents(pos_chunk):\n",
    "    #pos_chunk = pos_chunk[1:-1]\n",
    "    verbless_sents = 0\n",
    "    total_sents = 0\n",
    "    for sent in pos_chunk:\n",
    "        if len(sent) > 2:   #get rid of the non-sentences\n",
    "            verb_counter = 0\n",
    "            for tag in sent:\n",
    "                if tag[1] == 'V' or tag[1] == 'MD':\n",
    "                    verb_counter +=1\n",
    "            if verb_counter == 0:\n",
    "                verbless_sents +=1\n",
    "            total_sents += 1\n",
    "    if total_sents == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return verbless_sents/total_sents\n",
    "\n",
    "#returns average number of personal pronouns and possesive personal pronouns per sentence\n",
    "def per_pronoun_use(pos_chunk):\n",
    "    #pos_chunk = pos_chunk[1:-1]\n",
    "    sent_ratios = []\n",
    "    for sent in pos_chunk:\n",
    "        per_pronouns = 0\n",
    "        total_tags = 0\n",
    "        for tag in sent:\n",
    "            if tag[1] == 'PRP' or tag[1] == 'PRP$':\n",
    "                per_pronouns += 1\n",
    "            #don't include punctuation in your totals\n",
    "            if re.search(r'[A-Z$]+', tag[1]):\n",
    "                total_tags += 1\n",
    "        if total_tags != 0:\n",
    "            sent_ratios.append(per_pronouns/total_tags)   #do we normalize for sentence length in some other way?     \n",
    "    return np.mean(sent_ratios)                           #do we need to normalize for # of sentences in chunk?\n",
    "\n",
    "#get multiple type/token ratios for each chunk\n",
    "\n",
    "#cleaner functions\n",
    "def cleaner(text):\n",
    "    text = re.sub(r'[0-9]', '', text)\n",
    "    text = re.sub(r'[,.;:\"?!*()\\']', '', text)\n",
    "    text = re.sub(r'-', ' ', text)\n",
    "    text = re.sub(r'[\\n\\t]', ' ', text)\n",
    "    #text = remove_accents(text)\n",
    "    return text\n",
    "\n",
    "def rmv_stopwords(text):\n",
    "    chunk = nltk.word_tokenize(text)\n",
    "    new_chunk = []\n",
    "    for word in chunk:\n",
    "        if word not in jockers_stopwords:\n",
    "            new_chunk.append(word)\n",
    "    return ' '.join(new_chunk)\n",
    "\n",
    "def rmv_capitals(text):\n",
    "    chunk = nltk.sent_tokenize(text)\n",
    "    new_chunk = []\n",
    "    for sent in chunk:\n",
    "        if len(sent) > 1:         #avoid empty sentences\n",
    "            if sent[0].isupper():     #convert capitalized head-words to lowercase, since most we want to keep\n",
    "                sent[0].lower()\n",
    "            sent = re.sub(r'[A-Z][a-z\\'.]+', '', sent)   #gets rid of all capitalized words in a sentence; also Mr./Mrs.\n",
    "            new_chunk.append(sent)\n",
    "    return ' '.join(new_chunk)     #reconvert to a string\n",
    "\n",
    "#basic type/token ratio using all words in chunk\n",
    "def tt_ratio(chunk):\n",
    "    chunk = cleaner(chunk)   #gets rid of punctuation and numbers\n",
    "    chunk = nltk.word_tokenize(chunk.lower())\n",
    "    if len(set(chunk)) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return len(set(chunk))/len(chunk)   #compute the amount of lexical repetition\n",
    "\n",
    "#returns type/token ratio without stopwords\n",
    "def tt_ratio_no_stopwords(chunk):\n",
    "    chunk = cleaner(chunk)\n",
    "    chunk = rmv_stopwords(chunk.lower())\n",
    "    chunk = nltk.word_tokenize(chunk)\n",
    "    if len(set(chunk)) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return len(set(chunk))/len(chunk)\n",
    "\n",
    "#returns type/token ratio without stopwords or capitalized words\n",
    "def tt_ratio_no_capitals(chunk):\n",
    "    chunk = rmv_capitals(chunk)\n",
    "    chunk = rmv_stopwords(chunk.lower())\n",
    "    chunk = nltk.word_tokenize(chunk)\n",
    "    if len(set(chunk)) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return len(set(chunk))/len(chunk)\n",
    "\n",
    "# Onom detector and measurer\n",
    "with codecs.open('c:\\Users\\Hoyt\\Dropbox\\SOC_PROJECT\\ONOM_LIST.txt','r', 'utf-8') as f:\n",
    "    raw = f.read()\n",
    "    onom_list = re.split('\\n|\\r', raw)\n",
    "    \n",
    "def onom(chunk):\n",
    "    onoms = []\n",
    "    already_checked = []\n",
    "    chunk = rmv_capitals(chunk)\n",
    "    chunk = nltk.word_tokenize(cleaner(chunk))\n",
    "    #chunk = chunk[1:-1]\n",
    "    for word in chunk:\n",
    "        if word not in already_checked:\n",
    "            already_checked.append(word)\n",
    "            if word.lower() in onom_list:\n",
    "                onoms.append(word)\n",
    "    if len(already_checked) == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return len(set(onoms))/len(set(already_checked))\n",
    "\n",
    "#an efficient way to look up strings in a large list\n",
    "from bisect import bisect_left\n",
    "#the list here needs to be sorted\n",
    "def bi_contains(lst, item):\n",
    "    \"\"\" efficient `item in lst` for sorted lists \"\"\"\n",
    "    # if item is larger than the last its not in the list, but the bisect would \n",
    "    # find `len(lst)` as the index to insert, so check that first. Else, if the \n",
    "    # item is in the list then it has to be at index bisect_left(lst, item)\n",
    "    return (item <= lst[-1]) and (lst[bisect_left(lst, item)] == item)    \n",
    "    \n",
    "def neo(chunk):\n",
    "    neologisms_in_chunk = 0\n",
    "    num_words = 0\n",
    "    chunk = rmv_capitals(chunk)\n",
    "    chunk = nltk.word_tokenize(cleaner(chunk))\n",
    "    #chunk = chunk[1:-1]\n",
    "    for word in chunk:\n",
    "        if len(word) > 1:\n",
    "            if bi_contains(Xdict_entries, word) != True:   #if word not in the dictionary created above\n",
    "                neologisms_in_chunk += 1\n",
    "            num_words += 1 \n",
    "    if num_words == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return neologisms_in_chunk/num_words\n",
    "\n",
    "#function that calculates number of sentences per chunk starting with a PRP (should we exclude \"It\"?)\n",
    "def per_pronoun_head(pos_chunk):\n",
    "    #pos_chunk = pos_chunk[1:]     #leave the last sentence in, since we're looking at head words\n",
    "    prp_sents = 0\n",
    "    total_sents = 0\n",
    "    for sent in pos_chunk:\n",
    "        if len(sent) > 2:\n",
    "            tag_index = 0\n",
    "            #while tag not punct\n",
    "            while re.search('[^A-Z$]+', sent[tag_index][1]) and tag_index != (len(sent)-1): #or sent[tag_index][0] == '': \n",
    "                tag_index += 1   #step forward one element\n",
    "            total_sents += 1\n",
    "            if sent[tag_index][1] == 'PRP':\n",
    "                prp_sents += 1\n",
    "    if total_sents == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return prp_sents/total_sents\n",
    "\n",
    "#returns number of sentences per chunk that begin with obvious adverbial modification \n",
    "pos_list = [\"RB\", \"RP\", \"TO\", \"PRP$\", \"CC\", \"IN\", \"DT\", \"EX\"]\n",
    "word_list = [\"Is\", \"Had\", \"Said\", \"Says\", \"Come\", \"Looked\", \"Look\", \"Make\"] #NEED TO EXPAND THIS\n",
    "end_char_list = [\"g\", \"n\", \"d\"]     #This may be too restrictive, but catches most everything we want for now\n",
    "\n",
    "def adv_modification(pos_chunk):\n",
    "    #pos_chunk = pos_chunk[1:]   #leave the last sentence in, since we're looking at head words\n",
    "    adv_sents = 0\n",
    "    total_sents = 0             #for this total, will only include sents that start with capital letter \n",
    "    for sent in pos_chunk:\n",
    "        if len(sent) > 2:\n",
    "            #print sent[0:5]\n",
    "            tag_index = 0\n",
    "            #while tag not punct\n",
    "            while (re.search('[^A-Z$]+', sent[tag_index][1]) and tag_index != (len(sent)-1)) or sent[tag_index][0] == '': \n",
    "                tag_index += 1   #step forward one element\n",
    "            if re.search(r'[A-Z]', sent[tag_index][0][0]):     #only look at sentences that start with capital letter\n",
    "                total_sents += 1                               #count these toward the total\n",
    "                first_tag = sent[tag_index][1]\n",
    "                second_tag = sent[tag_index + 1][1]\n",
    "                first_word = sent[tag_index][0]\n",
    "                second_word = sent[tag_index + 1][0]\n",
    "                #print first_tag\n",
    "                #print second_tag\n",
    "                #check the starting sequence of words and tags\n",
    "                if first_tag == 'V' and first_word[-1] in end_char_list and second_tag in pos_list and second_word != 'n\\'t' :  \n",
    "                    adv_sents += 1\n",
    "    if total_sents == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return adv_sents/total_sents\n",
    "    \n",
    "#function to find ! or ? in non-dialogue passages\n",
    "def q_e(chunk):\n",
    "    chunk = nltk.sent_tokenize(chunk.encode('unicode_escape'))\n",
    "    partial_count = 0       #keep track of ! and ? that appear outside of dialogue\n",
    "    total_count = 0\n",
    "    in_quote = 0            #boolean variable to keep track of whether we're in or outside of quote\n",
    "    for sent in chunk:\n",
    "        if sent[0] == \"\\\"\" or sent[0] == \"\\'\":         #if starting item is quote mark, then we're in a quote\n",
    "            in_quote = 1\n",
    "        for char in sent:\n",
    "            if char == \"?\" or char == \"!\":        \n",
    "                if in_quote == 0:\n",
    "                    partial_count = partial_count + 1\n",
    "                    total_count = total_count + 1\n",
    "                elif in_quote == 1:\n",
    "                    total_count = total_count + 1      #this means we were in a quote, so just add to total\n",
    "        if sent[-1] == \"\\\"\" or sent[-1] == r'[.!?]\\'\\s': \n",
    "            in_quote = 0\n",
    "            partial_count = partial_count - 1    #this was actually a quote, so any ? or ! seen should not count\n",
    "    if partial_count > 0:\n",
    "        return partial_count/total_count    #ratio of q or e as function of total number of q or e\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "#function to find ellipses in passage\n",
    "def find_ellip(chunk):\n",
    "    count = len(re.findall(r'(\\.\\s){2}', chunk.encode('utf-8'))) + \\\n",
    "            len(re.findall(u\"…\", chunk.encode('utf-8'))) + \\\n",
    "            len(re.findall(r'(\\.\\.)+', chunk.encode('utf-8')))\n",
    "    return count/len(chunk) \n",
    "\n",
    "#going to have to update stop_words list to include personal names (if we are including bag-of-words)\n",
    "new_dir = 'c:\\Users\\Hoyt\\Dropbox\\SOC_PROJECT\\\\'\n",
    "#new_dir = '/Users/richardjeanso/Dropbox/SOC_PROJECT/'\n",
    "text = codecs.open(new_dir + \"jockers.txt\", \"r\", \"utf-8\")\n",
    "raw = text.read()\n",
    "#need to turn stopwords into a list\n",
    "raw = nltk.word_tokenize(raw)\n",
    "jockers_stopwords = []\n",
    "for word in raw:\n",
    "    jockers_stopwords.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added sent_length...\n",
      "added noun_ending...\n",
      "added verbless_sents...\n",
      "added per_pronouns...\n",
      "added per_pronoun_head...\n",
      "added adv_modification...\n",
      "added all tt_ratios...\n",
      "added onomotopoeia...\n",
      "added neologisms...\n",
      "added ellipses counts...\n",
      "added q_e counts...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300, 22)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load up the class labels and features into the data frame\n",
    "\n",
    "#always label soc as the \"0\" class -- all other classes will be labeled \"1\"\n",
    "soc_chunks_df['class_labels'] = [0]*len(soc_chunks_df)\n",
    "\n",
    "#this is only if you're interested in bag of words\n",
    "#vectorizer = TfidfVectorizer(min_df=4, stop_words=jockers_stopwords)\n",
    "#word_features = vectorizer.fit_transform(combined_df.chunk).toarray()\n",
    "#for i, col in enumerate(vectorizer.get_feature_names()):\n",
    "#    combined_df[col] = word_features[:,i]\n",
    "\n",
    "#all other features\n",
    "soc_chunks_df['sent_length'] = soc_chunks_df.chunk.map(lambda x: median_sent_length(x))\n",
    "print \"added sent_length...\"\n",
    "soc_chunks_df['noun_ending'] = soc_chunks_df.pos_chunks.map(lambda x: noun_ending(x))\n",
    "print \"added noun_ending...\"\n",
    "soc_chunks_df['verbless_sents'] = soc_chunks_df.pos_chunks.map(lambda x: verbless_sents(x))\n",
    "print \"added verbless_sents...\"\n",
    "soc_chunks_df['per_pronouns'] = soc_chunks_df.pos_chunks.map(lambda x: per_pronoun_use(x))\n",
    "print \"added per_pronouns...\"\n",
    "soc_chunks_df['per_pronoun_head'] = soc_chunks_df.pos_chunks.map(lambda x: per_pronoun_head(x)) \n",
    "print \"added per_pronoun_head...\"\n",
    "soc_chunks_df['adv_modification'] = soc_chunks_df.pos_chunks.map(lambda x: adv_modification(x)) \n",
    "print \"added adv_modification...\"\n",
    "soc_chunks_df['tt_ratio'] = soc_chunks_df.chunk.map(lambda x: tt_ratio(x))\n",
    "soc_chunks_df['tt_ratio_no_stopwords'] = soc_chunks_df.chunk.map(lambda x: tt_ratio_no_stopwords(x))\n",
    "soc_chunks_df['tt_ratio_no_capitals'] = soc_chunks_df.chunk.map(lambda x: tt_ratio_no_capitals(x))\n",
    "print \"added all tt_ratios...\"\n",
    "soc_chunks_df['onomotopoeia'] = soc_chunks_df.chunk.map(lambda x: onom(x))\n",
    "print \"added onomotopoeia...\"\n",
    "soc_chunks_df['neologisms'] = soc_chunks_df.chunk.map(lambda x: neo(x))\n",
    "print \"added neologisms...\"\n",
    "soc_chunks_df['ellipses'] = soc_chunks_df.chunk.map(lambda x: find_ellip(x))\n",
    "print \"added ellipses counts...\"\n",
    "soc_chunks_df['q_e'] = soc_chunks_df.chunk.map(lambda x: q_e(x))\n",
    "print \"added q_e counts...\"\n",
    "\n",
    "soc_chunks_df.to_pickle(\"c:\\Users\\Hoyt\\Dropbox\\SOC_TEXTS\\soc_df.pkl\")\n",
    "soc_chunks_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
